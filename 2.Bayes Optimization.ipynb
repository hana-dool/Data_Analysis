{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 우선 각 hyperparameter 의 의미를 파악한다. \n",
    "\n",
    "2. regularization 관련 / learning rate 관련 / tree 수 관련 등의 변수를 구분짓는다.\n",
    "\n",
    "3. default 설정에서의 loss 를 관찰하고 그것을 기준으로 삼는다.\n",
    "\n",
    "4. 각 값들을 넣어보면서 default 보다 좋은설정이 언제 나오는지 관찰한다. 이때에 큰 범주로 나눈 값들은 같이 변경하도록 한다.\n",
    "\n",
    "    ex) regulerization 을 넣어보고싶은 경우 관련 변수를 같이 조절하고, 비교가 되는 모델에서는 regularization 을 쓰지 않는다. \n",
    "\n",
    "    ex) reg 를 넣었을떄 좋은 값이 나왔다면 그 경우 reg 가 부족한것이므로 reg 를 더 올리면서 확인해본다..... 처럼 의미가 비슷한거끼리는 같이 영향이 상쇄되지 않게 조절해줍시다.\n",
    "\n",
    "5. 4번에서 어느정도 안정화가 되면, 그 값들을 범위를 설정하고, 베이지안 optimization 을 실행해본다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T09:25:37.526835Z",
     "start_time": "2020-08-02T09:25:35.250127Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics # 모델평가시 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T15:55:32.120523Z",
     "start_time": "2020-08-03T15:55:32.057769Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "california = fetch_california_housing()\n",
    "X = pd.DataFrame(california.data, columns=california.feature_names)\n",
    "y = pd.DataFrame(california.target,columns=[\"Target\"])\n",
    "df = pd.concat([X, y], axis=1)\n",
    "df.tail()\n",
    "y = california.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T09:25:37.637397Z",
     "start_time": "2020-08-02T09:25:37.607478Z"
    }
   },
   "outputs": [],
   "source": [
    "# dataset train/test set 으로 나누기\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest (회귀)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파라미터 범위 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_params = {\n",
    "    'min_samples_split' : (2,100),\n",
    "    'min_samples_leaf' : (1,100),\n",
    "    'max_depth': (1, 10),}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf_model=RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# 우선 우리가 tuning 하고 싶은 값들을 받고, 그에따라서 tuning 이 되게 조절해봅시다.\n",
    "# 우선 max_depth, min_samples_split, min_samples_leaf 를 이용한다고 해 봅시다.\n",
    "def ran_mse_eval(max_depth,\n",
    "                min_samples_split,\n",
    "                min_samples_leaf,):\n",
    "    # 여기에서는 우리가 조절해야할 파라미터들을 dic 형태로 정의합니다.\n",
    "    params = {\n",
    "        \"n_estimators\": 100 , # 굳이 parameter grid 로 찾고싶지는 않지만 ,default 값이 아니라 다른값을 주고싶을떄 이렇게 200으로 고정해서 하고싶다고 합시다!\n",
    "        'max_depth': int(round(max_depth)), # 이 때에 정수값이 들어가야하는 경우가 있는데요, 그 떄에는 int(round 로 처리를 해주어야 합니다.\n",
    "        'min_samples_split' : int(round(min_samples_split)),\n",
    "        'min_samples_leaf' : int(round(min_samples_leaf))}\n",
    "    print(\"params:\", params)  # 어떤 파라미터를 사용하였는지 print 하게 해서 학습과정을 지켜보도록 해요~\n",
    "    rf_model = RandomForestRegressor(**params) # 우리의 모델을 정의합니다.\n",
    "    \n",
    "    # 여기서 result 를 뽑아내기 위해서는 2가지 경우가 있습니다! \n",
    "    \n",
    "    # 1. y_val/ x_val 을 나누어서 평가하는 경우\n",
    "    #rf_model.fit(X_train, y_train) # 먼저 train 에 학습시킨 후\n",
    "    #valid_proba = rf_model.predict(X_val) # X_val 에 대해서 예측하고\n",
    "    #result = -1 * mean_squared_error(y_val, valid_proba) # 그에 따른 mse 스코어를 구합니다.\n",
    "    \n",
    "    # 하지만 위 같은 경우.. 뭔가 결과가 y_val/ x_val 에 과적합될 수도 있을거같아서 \n",
    "    # 2. 그냥 X_train/ y_train 의 cv 값을 하게되면 더 좋아보인다! \n",
    "    cv_value = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error') # 이떄 scoring 은 주최측에서 정한 값을 넣어야 게엣죠오?\n",
    "    result = np.mean(cv_value) # cv_value 는 list 형태로 나오게 되니까!\n",
    "    return result # 이 result 값이 커지게 베이지안optimization 이 학습하게 됩니다.\n",
    "\n",
    "    print('mse:', result)  # 그 값을 도출 print 해서 잘 학습하고 있는지 (줄여지는 방향으로) 알 아 보아요~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BayesianOptimization 객체를 생성합니다. \n",
    "이때 생성 인자로 앞에서 만든 평가함수 lgb_roc_eval 함수와 튜닝할 하이퍼 파라미터의 범위값을 설정한 딕셔너리 변수인 bayes_params를 입력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "# 객체를 형성한다\n",
    "BO_rf = BayesianOptimization(ran_mse_eval, bayes_params, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 입력받은 평가함수에 튜닝할 하이퍼 파라미터의 값을 반복적으로 입력하여 최적 하이퍼 파라미터를 튜닝할 준비가 되었습니다. \n",
    "BayesianOptimization객체에서 maximize()메소드를 호출하면 이를 수행할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | max_depth | min_sa... | min_sa... |\n",
      "-------------------------------------------------------------\n",
      "params: {'n_estimators': 100, 'max_depth': 6, 'min_samples_split': 61, 'min_samples_leaf': 72}\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.4154  \u001b[0m | \u001b[0m 5.939   \u001b[0m | \u001b[0m 71.8    \u001b[0m | \u001b[0m 61.07   \u001b[0m |\n",
      "params: {'n_estimators': 100, 'max_depth': 6, 'min_samples_split': 65, 'min_samples_leaf': 43}\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m-0.4083  \u001b[0m | \u001b[95m 5.904   \u001b[0m | \u001b[95m 42.94   \u001b[0m | \u001b[95m 65.3    \u001b[0m |\n",
      "params: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 96, 'min_samples_leaf': 89}\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-0.4525  \u001b[0m | \u001b[0m 4.938   \u001b[0m | \u001b[0m 89.29   \u001b[0m | \u001b[0m 96.44   \u001b[0m |\n",
      "params: {'n_estimators': 100, 'max_depth': 4, 'min_samples_split': 54, 'min_samples_leaf': 79}\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-0.5054  \u001b[0m | \u001b[0m 4.451   \u001b[0m | \u001b[0m 79.38   \u001b[0m | \u001b[0m 53.83   \u001b[0m |\n",
      "params: {'n_estimators': 100, 'max_depth': 6, 'min_samples_split': 9, 'min_samples_leaf': 93}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\bayes_opt\\target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: (6.112401049845391, 92.63406719097344, 8.96153370339292)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-148-834805d418c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mBO_rf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# 우리의 object function 을 maximize! 하려 한다. 그래서 위에서 return 을 negative mse 를 쓴 것이다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# 최적 파라미터 도출 작업을 n_iter 만큼 반복하여 수행합니다!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# init_points = 첫 시작지점의 score 를 5개 돌려서 알아보는것.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[1;34m(self, init_points, n_iter, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_bounds_transformer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params, lazy)\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOPTIMIZATION_STEP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\bayes_opt\\target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m             \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-146-70a8ac83ba14>\u001b[0m in \u001b[0;36mran_mse_eval\u001b[1;34m(max_depth, min_samples_split, min_samples_leaf)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m# 하지만 위 같은 경우.. 뭔가 결과가 y_val/ x_val 에 과적합될 수도 있을거같아서\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m# 2. 그냥 X_train/ y_train 의 cv 값을 하게되면 더 좋아보인다!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mcv_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrf_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'neg_mean_squared_error'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 이떄 scoring 은 주최측에서 정한 값을 넣어야 게엣죠오?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_value\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# cv_value 는 list 형태로 나오게 되니까!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;31m# 이 result 값이 커지게 베이지안optimization 이 학습하게 됩니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    404\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 406\u001b[1;33m                                 error_score=error_score)\n\u001b[0m\u001b[0;32m    407\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    246\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m             error_score=error_score)\n\u001b[1;32m--> 248\u001b[1;33m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1027\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1029\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    845\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 847\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    848\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    763\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    766\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 253\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 253\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    529\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    390\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m                     n_samples_bootstrap=n_samples_bootstrap)\n\u001b[1;32m--> 392\u001b[1;33m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[0;32m    393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m             \u001b[1;31m# Collect newly grown trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1032\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1033\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    845\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 847\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    848\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    763\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    766\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 253\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 253\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    166\u001b[0m                                                         indices=indices)\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1244\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1245\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1246\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m   1247\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    373\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BO_rf.maximize(init_points=5, n_iter=10) \n",
    "# 우리의 object function 을 maximize! 하려 한다. 그래서 위에서 return 을 negative mse 를 쓴 것이다.\n",
    "# 최적 파라미터 도출 작업을 n_iter 만큼 반복하여 수행합니다!\n",
    "# init_points = 첫 시작지점의 score 를 5개 돌려서 알아보는것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BayesianOptimization 객체의 res 속성은 하이퍼 파라미터 튜닝을 하는 과정에서의 metric 값과 그때의 하이퍼 파라미터 값을 가지고 있음. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BO_rf.res "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BayesianOptimization 객체의 max 속성은 최고 높은 성능 Metric를 가질때의 하이퍼 파라미터 값을 가지고 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "BO_rf.max\n",
    "# 이떄에 우리는 아래 값들을 반올림 해서(int(round)) 사용했음을 기억하세요!\n",
    "# 즉 max depth = 5 , min_sample_leaf = 3 , min_samples_split = 6 이 됩니다. # 이는 실행마다 달라지니까 주의하세용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최종 평가!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse score : 9.2232\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "max_params = BO_rf.max['params']\n",
    "max_params['min_samples_leaf'] = int(round(max_params['min_samples_leaf']))\n",
    "max_params['min_samples_split'] = int(round(max_params['min_samples_split']))\n",
    "model_rf = RandomForestRegressor(n_estimators=100, **max_params)\n",
    "model_rf.fit(X_train, y_train)\n",
    "y_pred = model_rf.predict(X_test)\n",
    "score = mean_squared_error(y_pred,y_test)\n",
    "\n",
    "print('mse score : {0:.4f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 비교 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "확실히... 그냥 search 보다는 우수한 성능을 보여주고있네요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model =  RandomForestRegressor(n_estimators=100)\n",
    "# scoring 은 default 이므로 model 의 자체 scoring 으로 들어간다. \n",
    "# n_estimator = 500 클수록 좋으나 내 컴퓨터가 버티질 못할듯.\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE : 9.294654162671725\n",
      "R_squared : 0.13772978443488426\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print (\"MSE :\", metrics.mean_squared_error(y_test, y_pred))\n",
    "print('R_squared :',model.score(X_test, y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBboost (회귀)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-1. 일반 파라메터\n",
    "\n",
    "\n",
    "(중요)booster: 어떤 부스터 구조를 쓸지 결정한다. 이것은 gbtree, gblinear, dart가 있다.\n",
    "\n",
    "(중요)n_eastimator : This is how many subtrees h will be trained.\n",
    "\n",
    "nthread: 몇 개의 쓰레드를 동시에 처리하도록 할지 결정한다. 디폴트는 “가능한 한 많이”.\n",
    "\n",
    "num_feature: feature 차원의 숫자를 정해야 하는 경우 옵션을 세팅한다. 디폴트는 “가능한 한 많이.”\n",
    "\n",
    "\n",
    "2-2. 부스팅 파라메터\n",
    "\n",
    "\n",
    "(중요)learning_rate: learning rate다. 트리에 가지가 많을수록 과적합하기 쉽다. 각 트리마다 가중치를 주어 부스팅 과정에 과적합이 일어나지 않도록 한다\n",
    "\n",
    "gamma: 정보 획득(Information Gain)에서 -r로 표현한 바 있다. 이것이 커지면, 트리 깊이가 줄어들어 보수적인 모델이 된다. 디폴트 값은 0이다\n",
    "\n",
    "(중요)max_depth: 한 트리의 maximum depth. 숫자를 키울수록 모델의 복잡도가 커진다. 작을수록 과적합 방지 . 디폴트는 6. \n",
    "\n",
    "lambda(L2 reg-form): L2 Regularization Form에 달리는 weights이다. 숫자가 클수록 보수적인 모델이 된다\n",
    "\n",
    "(중요)alpha(L1 reg-form): L1 Regularization Form weights다. 숫자가 클수록 과적합 방지\n",
    "\n",
    "\n",
    "2-3. 학습 과정 파라메터\n",
    "\n",
    "objective: 목적 함수다. reg:linear(linear-regression), binary:logistic(binary-logistic classification), count:poisson(count data poison regression) 등 다양하다\n",
    "\n",
    "eval_metric: 모델의 평가 함수를 조정하는 함수다. rmse(root mean square error), logloss(log-likelihood), map(mean average precision) 등, 해당 데이터의 특성에 맞게 평가 함수를 조정한다\n",
    "\n",
    "\n",
    "\n",
    "2-4. 커맨드 라인 파라메터\n",
    "num_rounds: boosting 라운드를 결정한다. 랜덤 하게 생성되는 모델이니만큼 이 수가 적당히 큰 게 좋다. epoch 옵션과 동일하다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "내 생각에 tuning 할때에는 중요 하다고 생각되는 것들을 신경쓰는게 좋아보인다.\n",
    "- 1. booster : 부스터 구조를 잘 골라서 잘 나오는것을 우선 선별한다.\n",
    "- 2. n_eastimator : 이 경우도 클수록 tree 가 많아져서 학습이 느려진다. 미리 선별해서 기억하는게 좋을듯하다.\n",
    "- 3. learning rate(트리간 가중치) / maxdepth(트리의 구조) / alpha(트리의 leaf 제한) 세개를 적절히 써서 잘 조절한다.\n",
    "- 4. 이제 어느정도 확정된 값을 가지고 어떻게 조절할지 감을 잡은 후에 colab 으로 Baysian 을 돌리면 될거같다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, colsample_bylevel=None,\n",
       "             colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
       "             gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
       "             learning_rate=None, max_delta_step=None, max_depth=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "             random_state=None, reg_alpha=None, reg_lambda=None,\n",
       "             scale_pos_weight=None, subsample=None, tree_method=None,\n",
       "             validate_parameters=None, verbosity=None)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "XGBRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파라미터 범위설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_params = {'n_estimators' : (100,300), # 트리의 갯수\n",
    "                'learning_rate' : (0.05,0.15), # learning rate\n",
    "                'max_depth': (9, 14), # 커질수록 복잡한 모델 (과적합 방지)\n",
    "                'reg_alpha': (0, 0.2), # leaves 에 대한 l1 규제의 계수이므로 클수록 규제\n",
    "                'colsample_bytree' :(0.85, 1.0)} # tree 에 사용되는 변수갯수 지정. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가 custom loss 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어떤 경우는 우리가 평가하고자 하는 loss 가 없을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "import numpy as np\n",
    "\n",
    "def mape(y_true, y_predict):\n",
    "    # Note this blows up if y_true = 0\n",
    "    # Ignore for demo -- in some sense an unsolvable\n",
    "    # problem with MAPE as an error metric \n",
    "    # 하지만 y_true 가 0 인경우는 없으므로 안심하라구!\n",
    "    y_true = np.array(y_true)\n",
    "    y_predict = np.array(y_predict)\n",
    "    return np.abs((y_true - y_predict)/y_true).mean()\n",
    "\n",
    "mape_scorer = make_scorer(mape, greater_is_better=False) \n",
    "# greater is better 이 false 이므로 neg 값이 나오게 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# 우선 우리가 tuning 하고 싶은 값들을 받고, 그에따라서 tuning 이 되게 조절해봅시다.\n",
    "# 우선 max_depth, min_samples_split, min_samples_leaf 를 이용한다고 해 봅시다.\n",
    "def ran_mape_eval(n_estimators,\n",
    "                  learning_rate,\n",
    "                 max_depth,\n",
    "                 reg_alpha,\n",
    "                 colsample_bytree):\n",
    "    # 여기에서는 우리가 조절해야할 파라미터들을 dic 형태로 정의합니다.\n",
    "    params = {\n",
    "        \"n_estimators\": int(round(n_estimators)) ,\n",
    "        'learning_rate': learning_rate,\n",
    "        'max_depth': int(round(max_depth)), # 이 때에 정수값이 들어가야하는 경우가 있는데요, 그 떄에는 int(round 로 처리를 해주어야 합니다.\n",
    "        'reg_alpha' : reg_alpha,\n",
    "        'colsample_bytree' : colsample_bytree}\n",
    "    print(\"params:\", params)  # 어떤 파라미터를 사용하였는지 print 하게 해서 학습과정을 지켜보도록 해요~\n",
    "    xgb_model = XGBRegressor(**params) # 우리의 모델을 정의합니다.\n",
    "    \n",
    "    # 여기서 result 를 뽑아내기 위해서는 2가지 경우가 있습니다! \n",
    "    \n",
    "    # 1. y_val/ x_val 을 나누어서 평가하는 경우\n",
    "    #rf_model.fit(X_train, y_train) # 먼저 train 에 학습시킨 후\n",
    "    #valid_proba = rf_model.predict(X_val) # X_val 에 대해서 예측하고\n",
    "    #result = -1 * mean_squared_error(y_val, valid_proba) # 그에 따른 mse 스코어를 구합니다.\n",
    "    \n",
    "    # 하지만 위 같은 경우.. 뭔가 결과가 y_val/ x_val 에 과적합될 수도 있을거같아서 \n",
    "    # 2. 그냥 X_train/ y_train 의 cv 값을 하게되면 더 좋아보인다! \n",
    "    cv_value = cross_val_score(xgb_model, X_train, y_train, cv=3, scoring = mape_scorer) # 이떄 scoring 은 우리가 위에서 정의한 평가함수를 썻다.\n",
    "    result = np.mean(cv_value) # cv_value 는 list 형태로 나오게 되니까!\n",
    "    return result # 이 result 값이 커지게 베이지안optimization 이 학습하게 됩니다.\n",
    "\n",
    "    print('mape:', result)  # 그 값을 도출 print 해서 잘 학습하고 있는지 (줄여지는 방향으로) 알 아 보아요~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BayesianOptimization 객체를 생성합니다. \n",
    "이때 생성 인자로 앞에서 만든 평가함수 lgb_roc_eval 함수와 튜닝할 하이퍼 파라미터의 범위값을 설정한 딕셔너리 변수인 bayes_params를 입력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "# 객체를 형성한다\n",
    "BO_rf = BayesianOptimization(ran_mape_eval, bayes_params, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 입력받은 평가함수에 튜닝할 하이퍼 파라미터의 값을 반복적으로 입력하여 최적 하이퍼 파라미터를 튜닝할 준비가 되었습니다. \n",
    "BayesianOptimization객체에서 maximize()메소드를 호출하면 이를 수행할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... | learni... | max_depth | n_esti... | reg_alpha |\n",
      "-------------------------------------------------------------------------------------\n",
      "params: {'n_estimators': 209, 'learning_rate': 0.12151893663724195, 'max_depth': 12, 'reg_alpha': 0.08473095986778095, 'colsample_bytree': 0.9323220255890987}\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.1702  \u001b[0m | \u001b[0m 0.9323  \u001b[0m | \u001b[0m 0.1215  \u001b[0m | \u001b[0m 12.01   \u001b[0m | \u001b[0m 209.0   \u001b[0m | \u001b[0m 0.08473 \u001b[0m |\n",
      "params: {'n_estimators': 293, 'learning_rate': 0.09375872112626925, 'max_depth': 13, 'reg_alpha': 0.07668830376515555, 'colsample_bytree': 0.9468841169599984}\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m-0.1696  \u001b[0m | \u001b[95m 0.9469  \u001b[0m | \u001b[95m 0.09376 \u001b[0m | \u001b[95m 13.46   \u001b[0m | \u001b[95m 292.7   \u001b[0m | \u001b[95m 0.07669 \u001b[0m |\n",
      "params: {'n_estimators': 285, 'learning_rate': 0.10288949197529045, 'max_depth': 12, 'reg_alpha': 0.014207211639577388, 'colsample_bytree': 0.9687587557123997}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\bayes_opt\\target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: (0.9687587557123997, 0.10288949197529045, 11.840222805469661, 285.1193276585322, 0.014207211639577388)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-155-a61174df166d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mBO_rf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# 우리의 object function 을 maximize! 하려 한다. 그래서 위에서 return 을 negative mse 를 쓴 것이다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# 최적 파라미터 도출 작업을 n_iter 만큼 반복하여 수행합니다!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# init_points = 첫 시작지점의 score 를 5개 돌려서 알아보는것.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[1;34m(self, init_points, n_iter, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_bounds_transformer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params, lazy)\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOPTIMIZATION_STEP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\bayes_opt\\target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m             \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-153-52fc72d070f4>\u001b[0m in \u001b[0;36mran_mape_eval\u001b[1;34m(n_estimators, learning_rate, max_depth, reg_alpha, colsample_bytree)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m# 하지만 위 같은 경우.. 뭔가 결과가 y_val/ x_val 에 과적합될 수도 있을거같아서\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# 2. 그냥 X_train/ y_train 의 cv 값을 하게되면 더 좋아보인다!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mcv_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmape_scorer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 이떄 scoring 은 우리가 위에서 정의한 평가함수를 썻다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_value\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# cv_value 는 list 형태로 나오게 되니까!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;31m# 이 result 값이 커지게 베이지안optimization 이 학습하게 됩니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    404\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 406\u001b[1;33m                                 error_score=error_score)\n\u001b[0m\u001b[0;32m    407\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    246\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m             error_score=error_score)\n\u001b[1;32m--> 248\u001b[1;33m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1027\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1029\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    845\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 847\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    848\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    763\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    766\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 253\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 253\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    529\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[0;32m    542\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 544\u001b[1;33m                               callbacks=callbacks)\n\u001b[0m\u001b[0;32m    545\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m    210\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[0mversion\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stat\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1367\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0;32m   1368\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1369\u001b[1;33m                                                     dtrain.handle))\n\u001b[0m\u001b[0;32m   1370\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1371\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_margin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BO_rf.maximize(init_points=5, n_iter=20) \n",
    "# 우리의 object function 을 maximize! 하려 한다. 그래서 위에서 return 을 negative mse 를 쓴 것이다.\n",
    "# 최적 파라미터 도출 작업을 n_iter 만큼 반복하여 수행합니다!\n",
    "# init_points = 첫 시작지점의 score 를 5개 돌려서 알아보는것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BayesianOptimization 객체의 res 속성은 하이퍼 파라미터 튜닝을 하는 과정에서의 metric 값과 그때의 하이퍼 파라미터 값을 가지고 있음. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BO_rf.res "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BayesianOptimization 객체의 max 속성은 최고 높은 성능 Metric를 가질때의 하이퍼 파라미터 값을 가지고 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': -0.16956449547695998,\n",
       " 'params': {'colsample_bytree': 0.9468841169599984,\n",
       "  'learning_rate': 0.09375872112626925,\n",
       "  'max_depth': 13.458865003910399,\n",
       "  'n_estimators': 292.73255210020585,\n",
       "  'reg_alpha': 0.07668830376515555}}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BO_rf.max\n",
    "# 이떄에 우리는 아래 값들을 반올림 해서(int(round)) 사용했음을 기억하세요!\n",
    "# 즉 max depth = 5 , min_sample_leaf = 3 , min_samples_split = 6 이 됩니다. # 이는 실행마다 달라지니까 주의하세용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최종 평가!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mape score : 0.1508\n"
     ]
    }
   ],
   "source": [
    "max_params = BO_rf.max['params']\n",
    "max_params['max_depth'] = int(round(max_params['max_depth']))\n",
    "max_params['n_estimators'] = int(round(max_params['n_estimators']))\n",
    "\n",
    "model_xgb = XGBRegressor(**max_params)\n",
    "model_xgb.fit(X_train, y_train)\n",
    "y_pred = model_xgb.predict(X_test)\n",
    "score = mape(y_pred,y_test)\n",
    "\n",
    "print('mape score : {0:.4f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 비교모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "             tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = XGBRegressor()\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE : 0.17614545366526785\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print (\"MAPE :\", mape(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Light LGB(회귀)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파라미터 범위설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGBMRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_params ={ \n",
    "    'learning_rate': (0.05,0.2),\n",
    "    'n_estimators' : (50,250),\n",
    "    'max_depth': (12,24), # 트리의 깊이\n",
    "    'num_leaves': (30,40), \n",
    "    'feature_fraction': (0.7,0.99), # 트리를 만들떄 얼마나 feaure 를 랜덤으로 선택할지\n",
    "    'bagging_fraction': (0.6,0.9), # 배깅의 비율\n",
    "    'bagging_freq':(1,3)} # 베깅을 얼마나 진행할지 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가 custom loss 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어떤 경우는 우리가 평가하고자 하는 loss 가 없을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "import numpy as np\n",
    "\n",
    "def mape(y_true, y_predict):\n",
    "    # Note this blows up if y_true = 0\n",
    "    # Ignore for demo -- in some sense an unsolvable\n",
    "    # problem with MAPE as an error metric \n",
    "    # 하지만 y_true 가 0 인경우는 없으므로 안심하라구!\n",
    "    y_true = np.array(y_true)\n",
    "    y_predict = np.array(y_predict)\n",
    "    return np.abs((y_true - y_predict)/y_true).mean()\n",
    "\n",
    "mape_scorer = make_scorer(mape, greater_is_better=False) \n",
    "# greater is better 이 false 이므로 neg 값이 나오게 된다. \n",
    "# 즉 MAPE SCORE 는 -mape 가 나와요~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#만일 shuffle 이 제대로 안되어있어서 걱정된다면 이렇게 k fold 로 shuffle 시켜주세요 ^^\n",
    "#from sklearn.model_selection import KFold\n",
    "#kfold = KFold(n_splits=5, shuffle=True, random_state=2)\n",
    "#cross_val_score(lgb_model, X_train, y_train, cv=kfold, scoring = mape_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def lgb_mape_eval(learning_rate,\n",
    "                  n_estimators,\n",
    "                 max_depth,\n",
    "                 num_leaves,\n",
    "                 feature_fraction,\n",
    "                 bagging_fraction,\n",
    "                 bagging_freq,\n",
    "                 ):\n",
    "    # 하이퍼 파라미터를 튜닝하기 위해 , 이게 제대로! 학습이 되고있는지 판단하기 위해 모델을 학습/ 평가하고 이에 따른 평가 지표를 반환하는 형식이 된다.\n",
    "    params = {\n",
    "        'learning_rate': learning_rate,\n",
    "        'max_depth': int(round(max_depth)), # 트리의 깊이\n",
    "        'num_leaves': int(round(num_leaves)), \n",
    "        'feature_fraction': feature_fraction, # 트리를 만들떄 얼마나 feaure 를 랜덤으로 선택할지\n",
    "        'bagging_fraction': bagging_fraction, # 배깅의 비율\n",
    "        'bagging_freq': int(round(bagging_freq)), # 베깅을 얼마나 진행할지\n",
    "        'n_estimators' : int(round(n_estimators))\n",
    "    }    \n",
    "\n",
    "    print(\"params:\", params)  # 어 떤 파라미터를 사용하였는지\n",
    "    lgb_model = LGBMRegressor(**params) # 모델! \n",
    "    cv_value = cross_val_score(lgb_model, X_train, y_train, cv=5, scoring = mape_scorer)\n",
    "    \n",
    "    result = np.mean(cv_value) \n",
    "    # cv_value 는 list 형태로 나오게 됩니다. (각 cv 마다의 score 값.)\n",
    "    return result \n",
    "    # 이 result 값이 커지게 베이지안optimization 이 학습하게 됩니다.\n",
    "\n",
    "    print('accuracy :', result)  # 그 값을 도출 print 해서 잘 학습하고 있는지 (줄여지는 방향으로) 알 아 보아요~    return roc_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BayesianOptimization 객체를 생성합니다. \n",
    "이때 생성 인자로 앞에서 만든 평가함수 lgb_roc_eval 함수와 튜닝할 하이퍼 파라미터의 범위값을 설정한 딕셔너리 변수인 bayes_params를 입력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "# 객체를 형성한다\n",
    "BO_rf = BayesianOptimization(lgb_mape_eval, bayes_params, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 입력받은 평가함수에 튜닝할 하이퍼 파라미터의 값을 반복적으로 입력하여 최적 하이퍼 파라미터를 튜닝할 준비가 되었습니다. \n",
    "BayesianOptimization객체에서 maximize()메소드를 호출하면 이를 수행할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | baggin... | featur... | learni... | max_depth | n_esti... | num_le... |\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "params: {'learning_rate': 0.13173247744953454, 'max_depth': 17, 'num_leaves': 34, 'feature_fraction': 0.8748013790607767, 'bagging_fraction': 0.7646440511781974, 'bagging_freq': 2, 'n_estimators': 179}\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.1706  \u001b[0m | \u001b[0m 0.7646  \u001b[0m | \u001b[0m 2.43    \u001b[0m | \u001b[0m 0.8748  \u001b[0m | \u001b[0m 0.1317  \u001b[0m | \u001b[0m 17.08   \u001b[0m | \u001b[0m 179.2   \u001b[0m | \u001b[0m 34.38   \u001b[0m |\n",
      "params: {'learning_rate': 0.1687587557123997, 'max_depth': 18, 'num_leaves': 39, 'feature_fraction': 0.8111980404594755, 'bagging_fraction': 0.867531900234624, 'bagging_freq': 3, 'n_estimators': 164}\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m-0.1704  \u001b[0m | \u001b[95m 0.8675  \u001b[0m | \u001b[95m 2.927   \u001b[0m | \u001b[95m 0.8112  \u001b[0m | \u001b[95m 0.1688  \u001b[0m | \u001b[95m 18.35   \u001b[0m | \u001b[95m 163.6   \u001b[0m | \u001b[95m 39.26   \u001b[0m |\n",
      "params: {'learning_rate': 0.17489297683219074, 'max_depth': 21, 'num_leaves': 40, 'feature_fraction': 0.7058633352576944, 'bagging_fraction': 0.6213108174593661, 'bagging_freq': 1, 'n_estimators': 224}\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-0.1775  \u001b[0m | \u001b[0m 0.6213  \u001b[0m | \u001b[0m 1.174   \u001b[0m | \u001b[0m 0.7059  \u001b[0m | \u001b[0m 0.1749  \u001b[0m | \u001b[0m 21.34   \u001b[0m | \u001b[0m 224.0   \u001b[0m | \u001b[0m 39.79   \u001b[0m |\n",
      "params: {'learning_rate': 0.06774116388033999, 'max_depth': 20, 'num_leaves': 39, 'feature_fraction': 0.926353461123072, 'bagging_fraction': 0.8397475692650171, 'bagging_freq': 2, 'n_estimators': 79}\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-0.1822  \u001b[0m | \u001b[0m 0.8397  \u001b[0m | \u001b[0m 1.923   \u001b[0m | \u001b[0m 0.9264  \u001b[0m | \u001b[0m 0.06774 \u001b[0m | \u001b[0m 19.68   \u001b[0m | \u001b[0m 78.67   \u001b[0m | \u001b[0m 39.45   \u001b[0m |\n",
      "params: {'learning_rate': 0.16613505341513252, 'max_depth': 17, 'num_leaves': 30, 'feature_fraction': 0.7767211275103418, 'bagging_fraction': 0.7565544965250215, 'bagging_freq': 2, 'n_estimators': 164}\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-0.173   \u001b[0m | \u001b[0m 0.7566  \u001b[0m | \u001b[0m 1.829   \u001b[0m | \u001b[0m 0.7767  \u001b[0m | \u001b[0m 0.1661  \u001b[0m | \u001b[0m 17.47   \u001b[0m | \u001b[0m 163.7   \u001b[0m | \u001b[0m 30.19   \u001b[0m |\n",
      "params: {'learning_rate': 0.05, 'max_depth': 24, 'num_leaves': 40, 'feature_fraction': 0.99, 'bagging_fraction': 0.6, 'bagging_freq': 3, 'n_estimators': 135}\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-0.1795  \u001b[0m | \u001b[0m 0.6     \u001b[0m | \u001b[0m 3.0     \u001b[0m | \u001b[0m 0.99    \u001b[0m | \u001b[0m 0.05    \u001b[0m | \u001b[0m 24.0    \u001b[0m | \u001b[0m 134.6   \u001b[0m | \u001b[0m 40.0    \u001b[0m |\n",
      "params: {'learning_rate': 0.05, 'max_depth': 24, 'num_leaves': 40, 'feature_fraction': 0.7, 'bagging_fraction': 0.9, 'bagging_freq': 3, 'n_estimators': 193}\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-0.1712  \u001b[0m | \u001b[0m 0.9     \u001b[0m | \u001b[0m 3.0     \u001b[0m | \u001b[0m 0.7     \u001b[0m | \u001b[0m 0.05    \u001b[0m | \u001b[0m 24.0    \u001b[0m | \u001b[0m 192.6   \u001b[0m | \u001b[0m 40.0    \u001b[0m |\n",
      "params: {'learning_rate': 0.12472102385409382, 'max_depth': 24, 'num_leaves': 40, 'feature_fraction': 0.7720763961650281, 'bagging_fraction': 0.8314376373392335, 'bagging_freq': 3, 'n_estimators': 173}\n",
      "| \u001b[95m 8       \u001b[0m | \u001b[95m-0.1695  \u001b[0m | \u001b[95m 0.8314  \u001b[0m | \u001b[95m 2.981   \u001b[0m | \u001b[95m 0.7721  \u001b[0m | \u001b[95m 0.1247  \u001b[0m | \u001b[95m 23.5    \u001b[0m | \u001b[95m 173.0   \u001b[0m | \u001b[95m 39.77   \u001b[0m |\n",
      "params: {'learning_rate': 0.09375052437923304, 'max_depth': 23, 'num_leaves': 39, 'feature_fraction': 0.9371506430319936, 'bagging_fraction': 0.8925919669634644, 'bagging_freq': 3, 'n_estimators': 174}\n",
      "| \u001b[95m 9       \u001b[0m | \u001b[95m-0.1687  \u001b[0m | \u001b[95m 0.8926  \u001b[0m | \u001b[95m 2.578   \u001b[0m | \u001b[95m 0.9372  \u001b[0m | \u001b[95m 0.09375 \u001b[0m | \u001b[95m 23.46   \u001b[0m | \u001b[95m 173.6   \u001b[0m | \u001b[95m 38.76   \u001b[0m |\n",
      "params: {'learning_rate': 0.05595332997717015, 'max_depth': 24, 'num_leaves': 30, 'feature_fraction': 0.7732915429010659, 'bagging_fraction': 0.818689843828635, 'bagging_freq': 1, 'n_estimators': 184}\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-0.1753  \u001b[0m | \u001b[0m 0.8187  \u001b[0m | \u001b[0m 1.12    \u001b[0m | \u001b[0m 0.7733  \u001b[0m | \u001b[0m 0.05595 \u001b[0m | \u001b[0m 23.97   \u001b[0m | \u001b[0m 184.4   \u001b[0m | \u001b[0m 30.21   \u001b[0m |\n",
      "params: {'learning_rate': 0.050459114886174466, 'max_depth': 12, 'num_leaves': 39, 'feature_fraction': 0.9736076245608813, 'bagging_fraction': 0.813305748333056, 'bagging_freq': 1, 'n_estimators': 172}\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-0.1752  \u001b[0m | \u001b[0m 0.8133  \u001b[0m | \u001b[0m 1.39    \u001b[0m | \u001b[0m 0.9736  \u001b[0m | \u001b[0m 0.05046 \u001b[0m | \u001b[0m 12.07   \u001b[0m | \u001b[0m 172.0   \u001b[0m | \u001b[0m 38.9    \u001b[0m |\n",
      "params: {'learning_rate': 0.06257526977942529, 'max_depth': 24, 'num_leaves': 33, 'feature_fraction': 0.7878086998962229, 'bagging_fraction': 0.677221679288176, 'bagging_freq': 2, 'n_estimators': 171}\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-0.1744  \u001b[0m | \u001b[0m 0.6772  \u001b[0m | \u001b[0m 2.395   \u001b[0m | \u001b[0m 0.7878  \u001b[0m | \u001b[0m 0.06258 \u001b[0m | \u001b[0m 23.75   \u001b[0m | \u001b[0m 170.8   \u001b[0m | \u001b[0m 32.61   \u001b[0m |\n",
      "params: {'learning_rate': 0.1046953001301889, 'max_depth': 23, 'num_leaves': 39, 'feature_fraction': 0.844141337224133, 'bagging_fraction': 0.7062916336088501, 'bagging_freq': 1, 'n_estimators': 180}\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m-0.1715  \u001b[0m | \u001b[0m 0.7063  \u001b[0m | \u001b[0m 1.177   \u001b[0m | \u001b[0m 0.8441  \u001b[0m | \u001b[0m 0.1047  \u001b[0m | \u001b[0m 22.76   \u001b[0m | \u001b[0m 179.5   \u001b[0m | \u001b[0m 39.23   \u001b[0m |\n",
      "params: {'learning_rate': 0.06719992961699134, 'max_depth': 19, 'num_leaves': 39, 'feature_fraction': 0.9506562232174386, 'bagging_fraction': 0.7142613973761445, 'bagging_freq': 2, 'n_estimators': 156}\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-0.1738  \u001b[0m | \u001b[0m 0.7143  \u001b[0m | \u001b[0m 1.943   \u001b[0m | \u001b[0m 0.9507  \u001b[0m | \u001b[0m 0.0672  \u001b[0m | \u001b[0m 19.44   \u001b[0m | \u001b[0m 156.0   \u001b[0m | \u001b[0m 38.87   \u001b[0m |\n",
      "params: {'learning_rate': 0.052247126952164546, 'max_depth': 19, 'num_leaves': 39, 'feature_fraction': 0.7991887891001287, 'bagging_fraction': 0.800485989461228, 'bagging_freq': 1, 'n_estimators': 175}\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-0.1732  \u001b[0m | \u001b[0m 0.8005  \u001b[0m | \u001b[0m 1.022   \u001b[0m | \u001b[0m 0.7992  \u001b[0m | \u001b[0m 0.05225 \u001b[0m | \u001b[0m 19.49   \u001b[0m | \u001b[0m 175.2   \u001b[0m | \u001b[0m 38.61   \u001b[0m |\n",
      "=============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "BO_rf.maximize(init_points=5, n_iter=10) \n",
    "# 우리의 object function 을 maximize! 하려 한다. 그래서 위에서 return 을 negative mse 를 쓴 것이다.\n",
    "# 최적 파라미터 도출 작업을 n_iter 만큼 반복하여 수행합니다!\n",
    "# init_points = 첫 시작지점의 score 를 5개 돌려서 알아보는것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BayesianOptimization 객체의 res 속성은 하이퍼 파라미터 튜닝을 하는 과정에서의 metric 값과 그때의 하이퍼 파라미터 값을 가지고 있음. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BO_rf.res "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BayesianOptimization 객체의 max 속성은 최고 높은 성능 Metric를 가질때의 하이퍼 파라미터 값을 가지고 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': -0.16873401068694988,\n",
       " 'params': {'bagging_fraction': 0.8925919669634644,\n",
       "  'bagging_freq': 2.5776214962985264,\n",
       "  'feature_fraction': 0.9371506430319936,\n",
       "  'learning_rate': 0.09375052437923304,\n",
       "  'max_depth': 23.46023058263993,\n",
       "  'n_estimators': 173.5539900082939,\n",
       "  'num_leaves': 38.75618804607675}}"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BO_rf.max\n",
    "# 이떄에 우리는 아래 값들을 반올림 해서(int(round)) 사용했음을 기억하세요!\n",
    "# 즉 max depth = 5 , min_sample_leaf = 3 , min_samples_split = 6 이 됩니다. # 이는 실행마다 달라지니까 주의하세용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최종 평가!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_params = BO_rf.max['params']\n",
    "max_params['max_depth'] = int(round(max_params['max_depth']))\n",
    "max_params['num_leaves'] = int(round(max_params['num_leaves']))\n",
    "max_params['bagging_freq']=int(round(max_params['bagging_freq']))\n",
    "max_params['n_estimators']=int(round(max_params['n_estimators']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mape : 0.1692\n"
     ]
    }
   ],
   "source": [
    "lgb_model = LGBMRegressor(**max_params)\n",
    "lgb_model.fit(X_train, y_train)\n",
    "y_pred = lgb_model.predict(X_test)\n",
    "score = mape(y_test,y_pred)\n",
    "\n",
    "print('mape : {0:.4f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 비교모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mape : 0.1600\n"
     ]
    }
   ],
   "source": [
    "lgb_model = LGBMRegressor()\n",
    "lgb_model.fit(X_train, y_train) \n",
    "y_pred = lgb_model.predict(X_test)\n",
    "score = mape(y_pred,y_test)\n",
    "\n",
    "print('mape : {0:.4f}'.format(score))\n",
    "\n",
    "# 조정값이 많아서 걍 DEFAULT 가 이긴모습... \n",
    "# 의외로 DEFAULT 이기기 쉽지않아요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Light LGB ( 분류 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_covtype\n",
    "covtype = fetch_covtype()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(covtype.data, \n",
    "                  columns=[\"x{:02d}\".format(i + 1) for i in range(covtype.data.shape[1])],\n",
    "                  dtype=int)\n",
    "y = covtype.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x01</th>\n",
       "      <th>x02</th>\n",
       "      <th>x03</th>\n",
       "      <th>x04</th>\n",
       "      <th>x05</th>\n",
       "      <th>x06</th>\n",
       "      <th>x07</th>\n",
       "      <th>x08</th>\n",
       "      <th>x09</th>\n",
       "      <th>x10</th>\n",
       "      <th>...</th>\n",
       "      <th>x45</th>\n",
       "      <th>x46</th>\n",
       "      <th>x47</th>\n",
       "      <th>x48</th>\n",
       "      <th>x49</th>\n",
       "      <th>x50</th>\n",
       "      <th>x51</th>\n",
       "      <th>x52</th>\n",
       "      <th>x53</th>\n",
       "      <th>x54</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2596</td>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>510</td>\n",
       "      <td>221</td>\n",
       "      <td>232</td>\n",
       "      <td>148</td>\n",
       "      <td>6279</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2590</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>212</td>\n",
       "      <td>-6</td>\n",
       "      <td>390</td>\n",
       "      <td>220</td>\n",
       "      <td>235</td>\n",
       "      <td>151</td>\n",
       "      <td>6225</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2804</td>\n",
       "      <td>139</td>\n",
       "      <td>9</td>\n",
       "      <td>268</td>\n",
       "      <td>65</td>\n",
       "      <td>3180</td>\n",
       "      <td>234</td>\n",
       "      <td>238</td>\n",
       "      <td>135</td>\n",
       "      <td>6121</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2785</td>\n",
       "      <td>155</td>\n",
       "      <td>18</td>\n",
       "      <td>242</td>\n",
       "      <td>118</td>\n",
       "      <td>3090</td>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "      <td>122</td>\n",
       "      <td>6211</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2595</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>153</td>\n",
       "      <td>-1</td>\n",
       "      <td>391</td>\n",
       "      <td>220</td>\n",
       "      <td>234</td>\n",
       "      <td>150</td>\n",
       "      <td>6172</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581007</th>\n",
       "      <td>2396</td>\n",
       "      <td>153</td>\n",
       "      <td>20</td>\n",
       "      <td>85</td>\n",
       "      <td>17</td>\n",
       "      <td>108</td>\n",
       "      <td>240</td>\n",
       "      <td>237</td>\n",
       "      <td>118</td>\n",
       "      <td>837</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581008</th>\n",
       "      <td>2391</td>\n",
       "      <td>152</td>\n",
       "      <td>19</td>\n",
       "      <td>67</td>\n",
       "      <td>12</td>\n",
       "      <td>95</td>\n",
       "      <td>240</td>\n",
       "      <td>237</td>\n",
       "      <td>119</td>\n",
       "      <td>845</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581009</th>\n",
       "      <td>2386</td>\n",
       "      <td>159</td>\n",
       "      <td>17</td>\n",
       "      <td>60</td>\n",
       "      <td>7</td>\n",
       "      <td>90</td>\n",
       "      <td>236</td>\n",
       "      <td>241</td>\n",
       "      <td>130</td>\n",
       "      <td>854</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581010</th>\n",
       "      <td>2384</td>\n",
       "      <td>170</td>\n",
       "      <td>15</td>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>90</td>\n",
       "      <td>230</td>\n",
       "      <td>245</td>\n",
       "      <td>143</td>\n",
       "      <td>864</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581011</th>\n",
       "      <td>2383</td>\n",
       "      <td>165</td>\n",
       "      <td>13</td>\n",
       "      <td>60</td>\n",
       "      <td>4</td>\n",
       "      <td>67</td>\n",
       "      <td>231</td>\n",
       "      <td>244</td>\n",
       "      <td>141</td>\n",
       "      <td>875</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>581012 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         x01  x02  x03  x04  x05   x06  x07  x08  x09   x10  ...  x45  x46  \\\n",
       "0       2596   51    3  258    0   510  221  232  148  6279  ...    0    0   \n",
       "1       2590   56    2  212   -6   390  220  235  151  6225  ...    0    0   \n",
       "2       2804  139    9  268   65  3180  234  238  135  6121  ...    0    0   \n",
       "3       2785  155   18  242  118  3090  238  238  122  6211  ...    0    0   \n",
       "4       2595   45    2  153   -1   391  220  234  150  6172  ...    0    0   \n",
       "...      ...  ...  ...  ...  ...   ...  ...  ...  ...   ...  ...  ...  ...   \n",
       "581007  2396  153   20   85   17   108  240  237  118   837  ...    0    0   \n",
       "581008  2391  152   19   67   12    95  240  237  119   845  ...    0    0   \n",
       "581009  2386  159   17   60    7    90  236  241  130   854  ...    0    0   \n",
       "581010  2384  170   15   60    5    90  230  245  143   864  ...    0    0   \n",
       "581011  2383  165   13   60    4    67  231  244  141   875  ...    0    0   \n",
       "\n",
       "        x47  x48  x49  x50  x51  x52  x53  x54  \n",
       "0         0    0    0    0    0    0    0    0  \n",
       "1         0    0    0    0    0    0    0    0  \n",
       "2         0    0    0    0    0    0    0    0  \n",
       "3         0    0    0    0    0    0    0    0  \n",
       "4         0    0    0    0    0    0    0    0  \n",
       "...     ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "581007    0    0    0    0    0    0    0    0  \n",
       "581008    0    0    0    0    0    0    0    0  \n",
       "581009    0    0    0    0    0    0    0    0  \n",
       "581010    0    0    0    0    0    0    0    0  \n",
       "581011    0    0    0    0    0    0    0    0  \n",
       "\n",
       "[581012 rows x 54 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.iloc[0:5000,:]\n",
    "y = y[0:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파라미터 범위 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_params = {\n",
    "    'num_leaves': (24, 45), # 범위값으로 인식하게 된다!\n",
    "    'colsample_bytree':(0.5, 1),  \n",
    "    'subsample': (0.5, 1),\n",
    "    'max_depth': (4, 12),\n",
    "    'reg_alpha': (0, 0.5),\n",
    "    'reg_lambda': (0, 0.5), \n",
    "    'min_split_gain': (0.001, 0.1),\n",
    "    'min_child_weight':(5, 50)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트 해볼 하이퍼 파라미터의 범위 값을 설정하였으면 BaysianOptimization에서 호출하여 모델을 최적화하는 함수를 만들어 보겠습니다.\n",
    "\n",
    "해당 함수는 BaysianOptimization에서 하이퍼 파라미터를 튜닝하기 위해 호출되면 제대로 튜닝이 되고 있는지를 판단하기 위해서 모델을 학습/평가하고 이에 따른 평가 지표를 반환하는 형식으로 만들어집니다. 이 평가 함수는 BayesianOptimization 객체에서 파라미터를 변경하면서 호출되므로 함수의 인자로 앞에서 딕셔너리로 설정된 파라미터들을 가지게 됩니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def lgb_acc_eval(num_leaves, \n",
    "                 colsample_bytree,\n",
    "                 subsample, \n",
    "                 max_depth, \n",
    "                 reg_alpha, \n",
    "                 reg_lambda, \n",
    "                 min_split_gain, \n",
    "                 min_child_weight):\n",
    "    # 하이퍼 파라미터를 튜닝하기 위해 , 이게 제대로! 학습이 되고있는지 판단하기 위해 모델을 학습/ 평가하고 이에 따른 평가 지표를 반환하는 형식이 된다.\n",
    "    params = {\n",
    "        \"n_estimator\":200,\n",
    "        \"learning_rate\":0.02,\n",
    "        'num_leaves': int(round(num_leaves)), # 이 값은 정수형을 return 받아야 하므로! round/ int 를 차례로 받는다.\n",
    "        'colsample_bytree': colsample_bytree, \n",
    "        'subsample': subsample,\n",
    "        'max_depth': int(round(max_depth)),\n",
    "        'reg_alpha': reg_alpha,\n",
    "        'reg_lambda': reg_lambda, \n",
    "        'min_split_gain': min_split_gain,\n",
    "        'min_child_weight': min_child_weight,\n",
    "        'verbosity': -1\n",
    "    }\n",
    "    print(\"params:\", params)  # 어 떤 파라미터를 사용하였는지\n",
    "    lgb_model = LGBMClassifier(**params) # 모델! \n",
    "    cv_value = cross_val_score(lgb_model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    \n",
    "    result = np.mean(cv_value) # cv_value 는 list 형태로 나오게 되니까!\n",
    "    return result # 이 result 값이 커지게 베이지안optimization 이 학습하게 됩니다.\n",
    "\n",
    "    print('accuracy :', result)  # 그 값을 도출 print 해서 잘 학습하고 있는지 (줄여지는 방향으로) 알 아 보아요~    return roc_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "# 객체를 형성한다\n",
    "BO_lgb = BayesianOptimization(lgb_acc_eval, bayes_params, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... | max_depth | min_ch... | min_sp... | num_le... | reg_alpha | reg_la... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 33, 'colsample_bytree': 0.7744067519636624, 'subsample': 0.9458865003910399, 'max_depth': 10, 'reg_alpha': 0.32294705653332806, 'reg_lambda': 0.21879360563134626, 'min_split_gain': 0.05494343511669279, 'min_child_weight': 32.12435192322397, 'verbosity': -1}\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.7729  \u001b[0m | \u001b[0m 0.7744  \u001b[0m | \u001b[0m 9.722   \u001b[0m | \u001b[0m 32.12   \u001b[0m | \u001b[0m 0.05494 \u001b[0m | \u001b[0m 32.9    \u001b[0m | \u001b[0m 0.3229  \u001b[0m | \u001b[0m 0.2188  \u001b[0m | \u001b[0m 0.9459  \u001b[0m |\n",
      "params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 36, 'colsample_bytree': 0.9818313802505146, 'subsample': 0.5435646498507704, 'max_depth': 7, 'reg_alpha': 0.4627983191463305, 'reg_lambda': 0.03551802909894347, 'min_split_gain': 0.05336059705553755, 'min_child_weight': 40.627626713719906, 'verbosity': -1}\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.7649  \u001b[0m | \u001b[0m 0.9818  \u001b[0m | \u001b[0m 7.068   \u001b[0m | \u001b[0m 40.63   \u001b[0m | \u001b[0m 0.05336 \u001b[0m | \u001b[0m 35.93   \u001b[0m | \u001b[0m 0.4628  \u001b[0m | \u001b[0m 0.03552 \u001b[0m | \u001b[0m 0.5436  \u001b[0m |\n",
      "params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 45, 'colsample_bytree': 0.5101091987201629, 'subsample': 0.8902645881432277, 'max_depth': 11, 'reg_alpha': 0.3995792821083618, 'reg_lambda': 0.23073968112646592, 'min_split_gain': 0.08713120267643511, 'min_child_weight': 40.01705379274327, 'verbosity': -1}\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.7594  \u001b[0m | \u001b[0m 0.5101  \u001b[0m | \u001b[0m 10.66   \u001b[0m | \u001b[0m 40.02   \u001b[0m | \u001b[0m 0.08713 \u001b[0m | \u001b[0m 44.55   \u001b[0m | \u001b[0m 0.3996  \u001b[0m | \u001b[0m 0.2307  \u001b[0m | \u001b[0m 0.8903  \u001b[0m |\n",
      "params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 35, 'colsample_bytree': 0.5591372129344666, 'subsample': 0.8871168447171083, 'max_depth': 9, 'reg_alpha': 0.2073309699952618, 'reg_lambda': 0.13227780605231348, 'min_split_gain': 0.09452222278790881, 'min_child_weight': 11.450897933407088, 'verbosity': -1}\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.8066  \u001b[0m | \u001b[95m 0.5591  \u001b[0m | \u001b[95m 9.119   \u001b[0m | \u001b[95m 11.45   \u001b[0m | \u001b[95m 0.09452 \u001b[0m | \u001b[95m 34.96   \u001b[0m | \u001b[95m 0.2073  \u001b[0m | \u001b[95m 0.1323  \u001b[0m | \u001b[95m 0.8871  \u001b[0m |\n",
      "params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 37, 'colsample_bytree': 0.7280751661082743, 'subsample': 0.8409101495517417, 'max_depth': 9, 'reg_alpha': 0.30846699843737846, 'reg_lambda': 0.4718740392573121, 'min_split_gain': 0.062145914210511834, 'min_child_weight': 5.845541019635982, 'verbosity': -1}\n",
      "| \u001b[95m 5       \u001b[0m | \u001b[95m 0.8169  \u001b[0m | \u001b[95m 0.7281  \u001b[0m | \u001b[95m 8.547   \u001b[0m | \u001b[95m 5.846   \u001b[0m | \u001b[95m 0.06215 \u001b[0m | \u001b[95m 36.85   \u001b[0m | \u001b[95m 0.3085  \u001b[0m | \u001b[95m 0.4719  \u001b[0m | \u001b[95m 0.8409  \u001b[0m |\n",
      "params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 24, 'colsample_bytree': 1.0, 'subsample': 0.5, 'max_depth': 4, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'min_split_gain': 0.001, 'min_child_weight': 5.0, 'verbosity': -1}\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.7671  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 4.0     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 24.0    \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.5     \u001b[0m |\n",
      "params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 45, 'colsample_bytree': 0.5, 'subsample': 1.0, 'max_depth': 12, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'min_split_gain': 0.1, 'min_child_weight': 6.153818666923531, 'verbosity': -1}\n",
      "| \u001b[95m 7       \u001b[0m | \u001b[95m 0.8274  \u001b[0m | \u001b[95m 0.5     \u001b[0m | \u001b[95m 12.0    \u001b[0m | \u001b[95m 6.154   \u001b[0m | \u001b[95m 0.1     \u001b[0m | \u001b[95m 45.0    \u001b[0m | \u001b[95m 0.0     \u001b[0m | \u001b[95m 0.0     \u001b[0m | \u001b[95m 1.0     \u001b[0m |\n",
      "params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 45, 'colsample_bytree': 1.0, 'subsample': 0.5, 'max_depth': 4, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'min_split_gain': 0.001, 'min_child_weight': 5.0, 'verbosity': -1}\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.7671  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 4.0     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 45.0    \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.5     \u001b[0m |\n",
      "params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 41, 'colsample_bytree': 0.5, 'subsample': 1.0, 'max_depth': 12, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'min_split_gain': 0.1, 'min_child_weight': 9.282792883879118, 'verbosity': -1}\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.8174  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 12.0    \u001b[0m | \u001b[0m 9.283   \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 40.95   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 45, 'colsample_bytree': 0.5, 'subsample': 1.0, 'max_depth': 12, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'min_split_gain': 0.1, 'min_child_weight': 16.37016837765438, 'verbosity': -1}\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.8006  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 12.0    \u001b[0m | \u001b[0m 16.37   \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 45.0    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 31, 'colsample_bytree': 0.9963415847423831, 'subsample': 0.8576257010583855, 'max_depth': 12, 'reg_alpha': 0.009330048227644716, 'reg_lambda': 0.019604314288185376, 'min_split_gain': 0.04155948070314624, 'min_child_weight': 5.033169891354441, 'verbosity': -1}\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.8266  \u001b[0m | \u001b[0m 0.9963  \u001b[0m | \u001b[0m 11.82   \u001b[0m | \u001b[0m 5.033   \u001b[0m | \u001b[0m 0.04156 \u001b[0m | \u001b[0m 31.37   \u001b[0m | \u001b[0m 0.00933 \u001b[0m | \u001b[0m 0.0196  \u001b[0m | \u001b[0m 0.8576  \u001b[0m |\n",
      "params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 24, 'colsample_bytree': 0.5, 'subsample': 0.5, 'max_depth': 12, 'reg_alpha': 0.28278185068333334, 'reg_lambda': 0.5, 'min_split_gain': 0.001, 'min_child_weight': 50.0, 'verbosity': -1}\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.7357  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 12.0    \u001b[0m | \u001b[0m 50.0    \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 24.0    \u001b[0m | \u001b[0m 0.2828  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.5     \u001b[0m |\n",
      "params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 24, 'colsample_bytree': 0.5, 'subsample': 0.5, 'max_depth': 12, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'min_split_gain': 0.001, 'min_child_weight': 18.488156702160605, 'verbosity': -1}\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.7926  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 12.0    \u001b[0m | \u001b[0m 18.49   \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 24.0    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.5     \u001b[0m |\n",
      "params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 37, 'colsample_bytree': 0.8958351898111097, 'subsample': 0.8153907933202891, 'max_depth': 12, 'reg_alpha': 0.019824024962597386, 'reg_lambda': 0.27977114529274616, 'min_split_gain': 0.02168998981760603, 'min_child_weight': 5.111223910488313, 'verbosity': -1}\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.8226  \u001b[0m | \u001b[0m 0.8958  \u001b[0m | \u001b[0m 11.91   \u001b[0m | \u001b[0m 5.111   \u001b[0m | \u001b[0m 0.02169 \u001b[0m | \u001b[0m 36.83   \u001b[0m | \u001b[0m 0.01982 \u001b[0m | \u001b[0m 0.2798  \u001b[0m | \u001b[0m 0.8154  \u001b[0m |\n",
      "params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 24, 'colsample_bytree': 1.0, 'subsample': 1.0, 'max_depth': 4, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'min_split_gain': 0.1, 'min_child_weight': 26.679596508578747, 'verbosity': -1}\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.7477  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 4.0     \u001b[0m | \u001b[0m 26.68   \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 24.0    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "=========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "BO_lgb.maximize(init_points=5, n_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BayesianOptimization 객체의 res 속성은 하이퍼 파라미터 튜닝을 하는 과정에서의 metric 값과 그때의 하이퍼 파라미터 값을 가지고 있음. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'target': 0.7728571428571429,\n",
       "  'params': {'colsample_bytree': 0.7744067519636624,\n",
       "   'max_depth': 9.721514930979357,\n",
       "   'min_child_weight': 32.12435192322397,\n",
       "   'min_split_gain': 0.05494343511669279,\n",
       "   'num_leaves': 32.896750786116996,\n",
       "   'reg_alpha': 0.32294705653332806,\n",
       "   'reg_lambda': 0.21879360563134626,\n",
       "   'subsample': 0.9458865003910399}},\n",
       " {'target': 0.7648571428571429,\n",
       "  'params': {'colsample_bytree': 0.9818313802505146,\n",
       "   'max_depth': 7.067532150606222,\n",
       "   'min_child_weight': 40.627626713719906,\n",
       "   'min_split_gain': 0.05336059705553755,\n",
       "   'num_leaves': 35.92893578297258,\n",
       "   'reg_alpha': 0.4627983191463305,\n",
       "   'reg_lambda': 0.03551802909894347,\n",
       "   'subsample': 0.5435646498507704}},\n",
       " {'target': 0.7594285714285715,\n",
       "  'params': {'colsample_bytree': 0.5101091987201629,\n",
       "   'max_depth': 10.660958764383505,\n",
       "   'min_child_weight': 40.01705379274327,\n",
       "   'min_split_gain': 0.08713120267643511,\n",
       "   'num_leaves': 44.55098518688804,\n",
       "   'reg_alpha': 0.3995792821083618,\n",
       "   'reg_lambda': 0.23073968112646592,\n",
       "   'subsample': 0.8902645881432277}},\n",
       " {'target': 0.8065714285714286,\n",
       "  'params': {'colsample_bytree': 0.5591372129344666,\n",
       "   'max_depth': 9.119368170620191,\n",
       "   'min_child_weight': 11.450897933407088,\n",
       "   'min_split_gain': 0.09452222278790881,\n",
       "   'num_leaves': 34.95881475675151,\n",
       "   'reg_alpha': 0.2073309699952618,\n",
       "   'reg_lambda': 0.13227780605231348,\n",
       "   'subsample': 0.8871168447171083}},\n",
       " {'target': 0.816857142857143,\n",
       "  'params': {'colsample_bytree': 0.7280751661082743,\n",
       "   'max_depth': 8.547471590949188,\n",
       "   'min_child_weight': 5.845541019635982,\n",
       "   'min_split_gain': 0.062145914210511834,\n",
       "   'num_leaves': 36.85401017717085,\n",
       "   'reg_alpha': 0.30846699843737846,\n",
       "   'reg_lambda': 0.4718740392573121,\n",
       "   'subsample': 0.8409101495517417}},\n",
       " {'target': 0.7671428571428571,\n",
       "  'params': {'colsample_bytree': 1.0,\n",
       "   'max_depth': 4.0,\n",
       "   'min_child_weight': 5.0,\n",
       "   'min_split_gain': 0.001,\n",
       "   'num_leaves': 24.0,\n",
       "   'reg_alpha': 0.5,\n",
       "   'reg_lambda': 0.5,\n",
       "   'subsample': 0.5}},\n",
       " {'target': 0.8274285714285714,\n",
       "  'params': {'colsample_bytree': 0.5,\n",
       "   'max_depth': 12.0,\n",
       "   'min_child_weight': 6.153818666923531,\n",
       "   'min_split_gain': 0.1,\n",
       "   'num_leaves': 45.0,\n",
       "   'reg_alpha': 0.0,\n",
       "   'reg_lambda': 0.0,\n",
       "   'subsample': 1.0}},\n",
       " {'target': 0.7671428571428571,\n",
       "  'params': {'colsample_bytree': 1.0,\n",
       "   'max_depth': 4.0,\n",
       "   'min_child_weight': 5.0,\n",
       "   'min_split_gain': 0.001,\n",
       "   'num_leaves': 45.0,\n",
       "   'reg_alpha': 0.5,\n",
       "   'reg_lambda': 0.5,\n",
       "   'subsample': 0.5}},\n",
       " {'target': 0.8174285714285714,\n",
       "  'params': {'colsample_bytree': 0.5,\n",
       "   'max_depth': 12.0,\n",
       "   'min_child_weight': 9.282792883879118,\n",
       "   'min_split_gain': 0.1,\n",
       "   'num_leaves': 40.95446342173249,\n",
       "   'reg_alpha': 0.0,\n",
       "   'reg_lambda': 0.0,\n",
       "   'subsample': 1.0}},\n",
       " {'target': 0.8005714285714285,\n",
       "  'params': {'colsample_bytree': 0.5,\n",
       "   'max_depth': 12.0,\n",
       "   'min_child_weight': 16.37016837765438,\n",
       "   'min_split_gain': 0.1,\n",
       "   'num_leaves': 45.0,\n",
       "   'reg_alpha': 0.0,\n",
       "   'reg_lambda': 0.0,\n",
       "   'subsample': 1.0}},\n",
       " {'target': 0.8265714285714285,\n",
       "  'params': {'colsample_bytree': 0.9963415847423831,\n",
       "   'max_depth': 11.819684742565805,\n",
       "   'min_child_weight': 5.033169891354441,\n",
       "   'min_split_gain': 0.04155948070314624,\n",
       "   'num_leaves': 31.371377406180006,\n",
       "   'reg_alpha': 0.009330048227644716,\n",
       "   'reg_lambda': 0.019604314288185376,\n",
       "   'subsample': 0.8576257010583855}},\n",
       " {'target': 0.7357142857142858,\n",
       "  'params': {'colsample_bytree': 0.5,\n",
       "   'max_depth': 12.0,\n",
       "   'min_child_weight': 50.0,\n",
       "   'min_split_gain': 0.001,\n",
       "   'num_leaves': 24.0,\n",
       "   'reg_alpha': 0.28278185068333334,\n",
       "   'reg_lambda': 0.5,\n",
       "   'subsample': 0.5}},\n",
       " {'target': 0.7925714285714286,\n",
       "  'params': {'colsample_bytree': 0.5,\n",
       "   'max_depth': 12.0,\n",
       "   'min_child_weight': 18.488156702160605,\n",
       "   'min_split_gain': 0.001,\n",
       "   'num_leaves': 24.0,\n",
       "   'reg_alpha': 0.0,\n",
       "   'reg_lambda': 0.0,\n",
       "   'subsample': 0.5}},\n",
       " {'target': 0.8225714285714286,\n",
       "  'params': {'colsample_bytree': 0.8958351898111097,\n",
       "   'max_depth': 11.912363935718016,\n",
       "   'min_child_weight': 5.111223910488313,\n",
       "   'min_split_gain': 0.02168998981760603,\n",
       "   'num_leaves': 36.82863912155457,\n",
       "   'reg_alpha': 0.019824024962597386,\n",
       "   'reg_lambda': 0.27977114529274616,\n",
       "   'subsample': 0.8153907933202891}},\n",
       " {'target': 0.7477142857142857,\n",
       "  'params': {'colsample_bytree': 1.0,\n",
       "   'max_depth': 4.0,\n",
       "   'min_child_weight': 26.679596508578747,\n",
       "   'min_split_gain': 0.1,\n",
       "   'num_leaves': 24.0,\n",
       "   'reg_alpha': 0.0,\n",
       "   'reg_lambda': 0.0,\n",
       "   'subsample': 1.0}}]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BO_lgb.res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BayesianOptimization 객체의 max 속성은 최고 높은 성능 Metric를 가질때의 하이퍼 파라미터 값을 가지고 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': 0.8274285714285714,\n",
       " 'params': {'colsample_bytree': 0.5,\n",
       "  'max_depth': 12.0,\n",
       "  'min_child_weight': 6.153818666923531,\n",
       "  'min_split_gain': 0.1,\n",
       "  'num_leaves': 45.0,\n",
       "  'reg_alpha': 0.0,\n",
       "  'reg_lambda': 0.0,\n",
       "  'subsample': 1.0}}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BO_lgb.max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최종 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse score : 0.8713\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "max_params = BO_rf.max['params']\n",
    "max_params['min_samples_leaf'] = int(round(max_params['min_samples_leaf']))\n",
    "max_params['min_samples_split'] = int(round(max_params['min_samples_split']))\n",
    "max_params['max_depth'] = int(max_params['max_depth'])\n",
    "lgb_model = LGBMClassifier(n_estimators=200,learning_rate=0.2, **max_params)\n",
    "lgb_model.fit(X_train, y_train) # train 을 전체로 해서 더 높은가바..\n",
    "y_pred = lgb_model.predict(X_test)\n",
    "score = accuracy_score(y_pred,y_test)\n",
    "\n",
    "print('accuracy : {0:.4f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catboost_eval(bagging_temperature ,\n",
    "                  depth , \n",
    "                  learning_rate ,\n",
    "                  min_data_in_leaf , \n",
    "                  max_leaves , \n",
    "                  l2_leaf_reg , \n",
    "                  border_count):\n",
    "    n_splits=5\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "    f1 = []\n",
    "    predict = None\n",
    "    params = {}\n",
    "    params['iterations'] = 1000\n",
    "    params['custom_loss'] = 'TotalF1'\n",
    "    params['eval_metric'] = 'TotalF1'\n",
    "    params['random_seed'] = 1234\n",
    "    params['learning_rate'] = learning_rate\n",
    "    params['min_data_in_leaf'] = int(round(min_data_in_leaf))\n",
    "    params['depth'] = int(round(depth))\n",
    "    #params['max_leaves'] = int(round(max_leaves))\n",
    "    #params['l2_leaf_reg'] = int(round(l2_leaf_reg))\n",
    "    params['border_count'] = int(round(border_count))\n",
    "    params['bagging_temperature'] = int(round(bagging_temperature))\n",
    "    X , y = catX_train.values , caty_train\n",
    "    for tr_ind, val_ind in skf.split(X , y):\n",
    "        X_train = X[tr_ind]\n",
    "        y_train = y[tr_ind]\n",
    "        X_valid = X[val_ind]\n",
    "        y_valid = y[val_ind]\n",
    "        ## https://catboost.ai/docs/concepts/python-reference_catboost_eval-metrics.html\n",
    "        clf = CatBoostClassifier(**params , \n",
    "                                 task_type = \"GPU\" , \n",
    "                                 leaf_estimation_iterations = 10,\n",
    "                                 use_best_model=True,\n",
    "                                 od_type=\"Iter\",\n",
    "                                 logging_level='Silent',\n",
    "                                )\n",
    "        clf.fit(X_train, \n",
    "                y_train,\n",
    "                cat_features=cat_features,\n",
    "                eval_set=(X_valid, y_valid),\n",
    "                verbose = False ,\n",
    "        )\n",
    "        \n",
    "        y_pred = clf.predict(X_valid)\n",
    "        \n",
    "        f1_value = f1_score(y_valid.astype(int) ,\n",
    "                            y_pred.astype(int)  ,\n",
    "                            average='weighted')\n",
    "        f1.append(f1_value)\n",
    "    return sum(f1)/n_splits\n",
    "\n",
    "## min_data_in_leaf , max_leaves , 추가 하니 먼가 잘 안됨.\n",
    "catBO = BayesianOptimization(catboost_eval,\n",
    "                             {'bagging_temperature': (0, 1000),\n",
    "                              'depth': (5, 8.99) ,\n",
    "                              \"learning_rate\" : (0.001,0.1) , \n",
    "                              \"min_data_in_leaf\" : (1,6) , \n",
    "                              'max_leaves' : (200,200)  ,\n",
    "                              'l2_leaf_reg': (100, 100)  ,\n",
    "                              'border_count': (5, 255) ,\n",
    "                             },\n",
    "                             random_state=0)\n",
    "init_round=5\n",
    "opt_round = 10\n",
    "catBO.maximize(init_points=init_round, n_iter=opt_round)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 변수 중요도 시각화\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "cat_feature_imp = pd.DataFrame([CatBoost.feature_names_ , CatBoost.feature_importances_]).T\n",
    "cat_feature_imp.columns = [\"feature\",\"varimp\"]\n",
    "cat_feature_imp = cat_feature_imp.sort_values(by=\"varimp\", ascending = False)\n",
    "sns.barplot(y=\"feature\", x=\"varimp\",data = cat_feature_imp, )\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "646.771px",
    "left": "964.986px",
    "top": "127.049px",
    "width": "290.226px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
